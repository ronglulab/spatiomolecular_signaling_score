{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da715332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import math\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import scipy\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Rectangle\n",
    "import networkx as nx\n",
    "from matplotlib_venn import venn3\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score, normalized_mutual_info_score, adjusted_mutual_info_score\n",
    "import warnings\n",
    "from fastcluster import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram, set_link_color_palette\n",
    "from matplotlib.colors import rgb2hex, colorConverter\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corr_graph(hspc_type, threshold, absolute = False, just_positive = False):\n",
    "    #correlations_pivot = pd.read_csv(\"pathway_correlations/\" + hspc_type + \"_correlations_pivot.csv\", index_col = 0)\n",
    "    correlations_pivot = pd.read_csv(\"pseudotime_lin_temp/\" + hspc_type + \"_correlations_pivot_ZSCORE.csv\", index_col = 0)\n",
    "    \n",
    "    links = correlations_pivot.stack().reset_index()\n",
    "    links.columns = ['Pathway 1', 'Pathway 2', 'Value']\n",
    "    links = links[ abs(links['Value']) > threshold ]\n",
    "    \n",
    "    if just_positive:\n",
    "        links = links[ links['Value'] > 0 ]\n",
    "    \n",
    "    if absolute:\n",
    "        links['Value'] = abs(links[\"Value\"])\n",
    "    \n",
    "    graph = nx.from_pandas_edgelist(links, 'Pathway 1', 'Pathway 2', ['Value'])\n",
    "    return graph\n",
    "\n",
    "# Degree centrality (G::graph, N::node) <- Degree(N)/#(G)-1\n",
    "# Degree(G::graph, N::node) <- Number of connections N has\n",
    "\n",
    "def get_degree_centrality(hspc_type, threshold ,absolute=True):\n",
    "    #print(hspc_type)\n",
    "    graph = make_corr_graph(hspc_type, threshold, absolute)\n",
    "    total_degrees = sum(dict(graph.degree()).values())\n",
    "    \n",
    "    \n",
    "    # Fraction of the node its connected to\n",
    "    degree_ratio = nx.degree_centrality(graph)\n",
    "    # instead, should we be consdiering closeness centrality? This \n",
    "    # would take into account the actual correlation values\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        data['distance'] = 1 / data['Value']\n",
    "    \n",
    "    degree_ratio = nx.closeness_centrality(graph, distance = 'distance')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    node_prop_df = pd.DataFrame.from_dict(degree_ratio.items())\n",
    "    if (len(node_prop_df) > 0):\n",
    "        node_prop_df.columns = ['Pathway name', 'degree proportion']\n",
    "        node_prop_df = node_prop_df.set_index('Pathway name')\n",
    "        \n",
    "    else:\n",
    "        node_prop_df = pd.DataFrame()\n",
    "        node_prop_df['Pathway name'] = [0]\n",
    "        node_prop_df['degree proportion'] = [0]\n",
    "        node_prop_df = node_prop_df.set_index('Pathway name')\n",
    "    return node_prop_df\n",
    "\n",
    "def filter_out_rows(df, threshold):\n",
    "    row_max = df.max(axis = 1)\n",
    "    keep = row_max[row_max >= threshold].index\n",
    "    return df.loc[keep]\n",
    "\n",
    "def filter_rows_rank(df, threshold, n = 40):\n",
    "    row_max_names = df.max(axis = 1).sort_values(ascending = False).index[:n]\n",
    "    return df.loc[row_max_names]\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "def cluster_stability(X, est, n_iter=200, random_state=None):\n",
    "    labels = []\n",
    "    indices = []\n",
    "    for i in range(n_iter):\n",
    "        # draw bootstrap samples, store indices\n",
    "        sample_indices = rng.randint(0, X.shape[0], X.shape[0])\n",
    "        indices.append(sample_indices)\n",
    "        est = clone(est)\n",
    "        X_bootstrap = X.iloc[sample_indices]\n",
    "        est.fit(X_bootstrap)\n",
    "        # store clustering outcome using original indices\n",
    "        relabel = -np.ones(X.shape[0])\n",
    "        relabel[sample_indices] = est.labels_\n",
    "        labels.append(relabel)\n",
    "    scores = []\n",
    "    for l, i in zip(labels, indices):\n",
    "        for k, j in zip(labels, indices):\n",
    "            # we also compute the diagonal which is a bit silly\n",
    "            in_both = np.intersect1d(i, j)\n",
    "            scores.append(adjusted_rand_score(l[in_both], k[in_both]))\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_mathdefault_num(s):\n",
    "    start = '$\\\\mathdefault{'\n",
    "    end = '}$'\n",
    "    return s[s.find(start)+len(start):s.rfind(end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd39581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centrality_clustered_df(lineage, corr_threshold = 0.1, \n",
    "                              degree_centrality_threshold = 0.1\n",
    "                             ):\n",
    "    degree_node_centrality = []\n",
    "\n",
    "    for i in lineage:\n",
    "        degree_node_centrality.append(get_degree_centrality(i, corr_threshold))\n",
    "        \n",
    "    hspc_degree_centrality = pd.concat(degree_node_centrality, axis = 1)\n",
    "    hspc_degree_centrality.columns = [str(i) for i in range(0,len(hspc_degree_centrality.columns))]\n",
    "    hspc_degree_centrality = hspc_degree_centrality.fillna(0)\n",
    "    hspc_degree_centrality = filter_out_rows(hspc_degree_centrality, degree_centrality_threshold)\n",
    "    \n",
    "    if 0 in hspc_degree_centrality.index:\n",
    "        hspc_degree_centrality = hspc_degree_centrality.drop(0) \n",
    "    \n",
    "    #return hspc_degree_centrality\n",
    "    \n",
    "    cg = sns.clustermap(hspc_degree_centrality, yticklabels = hspc_degree_centrality.index,\n",
    "                    xticklabels = [], metric = 'correlation',\n",
    "                     linewidth = 0.1, linecolor = 'k', \n",
    "                    square = True, cmap = 'gist_heat_r',\n",
    "                    col_cluster = False, row_cluster = True)\n",
    "    \n",
    "    clustering_order = [i.get_text() for i in cg.ax_heatmap.yaxis.get_majorticklabels()]\n",
    "    hspc_degree_centrality = hspc_degree_centrality.T\n",
    "    hspc_degree_centrality = hspc_degree_centrality[clustering_order]\n",
    "    hspc_degree_centrality = hspc_degree_centrality.T\n",
    "    \n",
    "    return hspc_degree_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_centrality_and_weights(centrality_dict, lineage, corr_threshold = 0.1, \n",
    "                              degree_centrality_threshold = 0.1,\n",
    "                               lineage_name = 'test_corr', sww = None):\n",
    "    \n",
    "    \n",
    "    \n",
    "    ending_early_frame = 50\n",
    "    ending_next_frame = 112\n",
    "    \n",
    "    \n",
    "    lineage_number = lineage_name.split('n')[1]\n",
    "    peak2_windows = centrality_dict[lineage_number]['peak2_windows']\n",
    "    peak3_windows = centrality_dict[lineage_number]['peak3_windows']\n",
    "    \n",
    "    # GET STEP AND WINDOW SIZE FROM FILE NAMES\n",
    "    step_size = lineage[0].split('_')[3]\n",
    "    window_size = lineage[0].split('_')[4]\n",
    "    \n",
    "    # PREPARE PLOTS...\n",
    "    fig, ((ax1, cbar_ax), (ax2, dummy_ax)) = plt.subplots(nrows=2, ncols=2, figsize=(4, 7),\n",
    "                                                          sharex='col',\n",
    "                                                      gridspec_kw={'height_ratios': [6, 30],\n",
    "                                                                   'width_ratios': [20, 1]})\n",
    "    \n",
    "    # CACLUATE THE CENTRALITY OF EACH WINDOW\n",
    "    centrality_df = get_centrality_clustered_df(lineage, corr_threshold = corr_threshold, \n",
    "                              degree_centrality_threshold = degree_centrality_threshold\n",
    "                             )\n",
    "    \n",
    "    # PLOT THE CENTRALITY\n",
    "    heatmap = sns.heatmap(centrality_df, cmap=\"gist_heat_r\", cbar_ax=dummy_ax,\n",
    "                          xticklabels=False, linewidths=0, \n",
    "                ax=ax2, yticklabels = False, linecolor = 'k',cbar_kws={'label': 'Pathway Cross-Talk'})\n",
    "    heatmap.add_patch(Rectangle((0,0), len(centrality_df.columns), len(centrality_df.index), fill = False,\n",
    "                               edgecolor = 'black', lw = 1))\n",
    "    ax2.set_xlabel('Pseudotime ' + \"\\u2192\")\n",
    "    ax2.set_ylabel('Pathway')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # GET THE AVERAGE WEIGHT PER WINDOW\n",
    "    if sww == None:\n",
    "        sliding_window_weight = sliding_window_weights(lineage_name,\n",
    "                                                   window_size = int(window_size),\n",
    "                                                   step_size = int(step_size))\n",
    "    else:\n",
    "        sliding_window_weight = sww\n",
    "    \n",
    "    # PLOT THE WINDOW WEIGHT\n",
    "    x = [i + 0.5 for i in np.array(range(len(centrality_df.columns)))]\n",
    "    y = sliding_window_weight\n",
    "    print(len(y))\n",
    "    ax1.plot(x, y, c='#3b3bc4')\n",
    "    \n",
    "    # PLOT THE AVERAGE WINDOW CROSS-TALK\n",
    "    axTwin = ax1.twinx()\n",
    "    x_mean = np.array(range(len(centrality_df.columns)))\n",
    "    y_mean = centrality_df.mean()\n",
    "    # Create a smoother curve\n",
    "    z = np.polyfit(x_mean,\n",
    "                 y_mean, 20)\n",
    "    polyfitted = np.poly1d(z)\n",
    "    x_smooth = [i + 0.5 for i in x_mean]\n",
    "    #axTwin.plot(x_smooth, polyfitted(x_smooth), c = '#c93853')\n",
    "    axTwin.plot(x_smooth, y_mean, c = '#c93853')\n",
    "    \n",
    "    # TOUCH UP THE FIGURE\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    axTwin.spines['top'].set_visible(False)\n",
    "    \n",
    "    # PLOT THE EARLY CENTRALITY PEAK WINDOW BOUNDS...\n",
    "    #ax1.axvline(x = ending_early_frame, linestyle = 'dotted', c= '#F5B700', linewidth = 2)\n",
    "    #ax1.axvline(x = ending_next_frame, linestyle = 'dotted', c= '#008bf8', linewidth = 2)\n",
    "    \n",
    "    ax1.axvline(x = min(peak2_windows), linestyle = 'dotted', c= 'black', linewidth = 2)\n",
    "    ax1.axvline(x = max(peak2_windows), linestyle = 'dotted', c= 'black', linewidth = 2)\n",
    "    \n",
    "    ax1.axvline(x = min(peak3_windows), linestyle = 'dotted', c= 'purple', linewidth = 2)\n",
    "    ax1.axvline(x = max(peak3_windows), linestyle = 'dotted', c= 'purple', linewidth = 2)\n",
    "    \n",
    "    ax1.set_ylabel('Mean Lineage\\nCommitment Score', c = '#3b3bc4')\n",
    "    axTwin.set_ylabel('Mean Pathway\\nCross-Talk', c = '#c93853')\n",
    "    cbar_ax.axis('off')\n",
    "    \n",
    "    cbar = heatmap.collections[0].colorbar\n",
    "    cbar_ax = cbar.ax\n",
    "    for spine in cbar_ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set(lw = 1, edgecolor = 'k')\n",
    "    \n",
    "    #pos1 = ax1.get_position()  # Get the original position\n",
    "    #pos2 = ax_celltype.get_position()\n",
    "    #gap = pos2.y1 - pos1.y0     # Calculate the gap between the subplots\n",
    "    #ax1.set_position([pos1.x0 , pos1.y0 + gap, pos1.width, pos1.height])\n",
    "    \n",
    "    #pos1 = ax_celltype.get_position()  # Get the original position\n",
    "    #pos2 = ax2.get_position()\n",
    "    #gap = pos2.y1 - pos1.y0     # Calculate the gap between the subplots\n",
    "    #ax2.set_position([pos2.x0 , pos2.y0 - gap, pos2.width, pos2.height])\n",
    "    #pos3 = dummy_ax.get_position()\n",
    "    #dummy_ax.set_position([pos3.x0 , pos3.y0 - gap, pos3.width, pos3.height])\n",
    "    \n",
    "    fig.set_size_inches(5,6)\n",
    "    plt.show()\n",
    "    \n",
    "    return centrality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b635da75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_celltypes(lineage_name, window_size = 100, step_size = 10):\n",
    "    weightDF = pd.read_csv(\"../hspc_pseudotime_metacell_\" + lineage_name + \"_metadata_weights_v2_ALL_CELLS.csv\", index_col = 0)\n",
    "    weightDF = weightDF.sort_values(by = 'median_pseudotime')\n",
    "\n",
    "    sliding_window_weights = pd.DataFrame()\n",
    "    \n",
    "    starting_ind, ending_ind = 0, window_size\n",
    "    \n",
    "    while ending_ind <= len(weightDF):\n",
    "        \n",
    "        pseudotime_window_sub = weightDF.iloc[list(range(starting_ind,ending_ind)), ]\n",
    "        pseudotime_window_sub = pseudotime_window_sub[['singlecelltype_HSC', 'singlecelltype_MPP.Flk2n', 'singlecelltype_MPP.Flk2p',\n",
    "                                                       'singlecelltype_CMP', \n",
    "                                                      'singlecelltype_CLP', 'singlecelltype_GMP', 'singlecelltype_MEP', \n",
    "                                                      'singlecelltype_MkP']]\n",
    "        \n",
    "        celltype_fraction = pseudotime_window_sub.sum(axis = 0)\n",
    "        \n",
    "        sliding_window_weights[starting_ind] = celltype_fraction\n",
    "        \n",
    "        starting_ind += step_size\n",
    "        ending_ind += step_size\n",
    "        \n",
    "    return sliding_window_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7399f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_weights(lineage_name, window_size = 100, step_size = 10, subsampling = 0):\n",
    "    \n",
    "    \n",
    "    weightDF = pd.read_csv(\"../hspc_pseudotime_metacell_\" + str(lineage_name) + \"_metadata_weights_v2_ALL_CELLS.csv\")\n",
    "\n",
    "    if subsampling > 0:\n",
    "        subsample_ids = pd.read_csv(\"../hspc_pseudotime_metacell_\" + str(lineage_name) + \"_subsample_ID_\" + str(subsampling) + \".csv\",\n",
    "                                    index_col = 0)['ids']\n",
    "        weightDF = weightDF.loc[subsample_ids,]\n",
    "        \n",
    "    weightDF = weightDF.sort_values(by = 'median_pseudotime')\n",
    "    \n",
    "    pseudotime_weights = list(weightDF['mean_pseudotime_weight'])\n",
    "\n",
    "    sliding_window_weights = []\n",
    "    \n",
    "    starting_ind, ending_ind = 0, window_size\n",
    "    \n",
    "    while ending_ind <= len(pseudotime_weights):\n",
    "        \n",
    "        pseudotime_window_weights = pseudotime_weights[starting_ind:ending_ind]\n",
    "        \n",
    "        sliding_window_weights.append(sum(pseudotime_window_weights)/window_size)\n",
    "        \n",
    "        starting_ind += step_size\n",
    "        ending_ind += step_size\n",
    "        \n",
    "    return sliding_window_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d7314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORR_THRESHOLD = 1.96 # this is a z-score threshold...\n",
    "CENTRALITY_THRESHOLD = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lineage1 = ['lin1_v2_allcells_downsampled_608_' + str(i) + \"_10_200\" for i in range(0,2630,10)]\n",
    "lineage1 = ['lin1_v2_allcells_downsampled_608_' + str(i) + \"_20_500\" for i in range(0,2330,20)]\n",
    "lineage2 = ['lin2_v2_allcells_downsampled_624_' + str(i) + \"_20_500\" for i in range(0,1850,20)]\n",
    "lineage3 = ['lin3_v2_allcells_downsampled_362_' + str(i) + \"_20_500\" for i in range(0,1230,20)]\n",
    "lineage4 = ['lin4_v2_allcells_downsampled_197_' + str(i) + \"_20_500\" for i in range(0,450,20)]\n",
    "\n",
    "# get the subsampled cell IDs\n",
    "# sliding_window_weight function for subsampled group...\n",
    "\n",
    "subsample_numbers = {'1':608,\n",
    "                     '2':624,\n",
    "                     '3':362,\n",
    "                     '4':197\n",
    "                     }\n",
    "\n",
    "lineage_peak1_windows = {'1' : range(0,1),\n",
    "                        '2' : range(0,1),\n",
    "                        '3' : range(0,1),\n",
    "                        '4' : range(0,1)}\n",
    "\n",
    "lineage_peak2_windows = {'1' : range(5,25),\n",
    "                        '2' : range(1,21),\n",
    "                        '3' : range(2,18),\n",
    "                        '4' : range(0,9)}\n",
    "\n",
    "lineage_peak3_windows = {'1' : range(48,73),\n",
    "                        '2' : range(37,57),\n",
    "                        '3' : range(27,44),\n",
    "                        '4' : range(13,20)}\n",
    "\n",
    "\n",
    "lineages = [lineage1, lineage2, lineage3, lineage4]\n",
    "lineage_ids = ['1', '2', '3', '4']\n",
    "\n",
    "lineage_name = {'1' : 'GMP/Gran. Lineage',\n",
    "               '2' : 'MEP Lineage',\n",
    "                '3' : 'GMP/Mono. Lineage',\n",
    "               '4' : 'Lymphoid Lineage'}\n",
    "\n",
    "centrality_dict_downsampled = {}\n",
    "\n",
    "for lineage, lineage_id in zip(lineages, lineage_ids):\n",
    "    centrality_dict_downsampled[lineage_id] = {}\n",
    "    centrality_dict_downsampled[lineage_id]['lineage_name'] = lineage_name[lineage_id]\n",
    "    centrality_dict_downsampled[lineage_id]['peak1_windows'] = lineage_peak1_windows[lineage_id]\n",
    "    centrality_dict_downsampled[lineage_id]['peak2_windows'] = lineage_peak2_windows[lineage_id]\n",
    "    centrality_dict_downsampled[lineage_id]['peak3_windows'] = lineage_peak3_windows[lineage_id]\n",
    "    \n",
    "    sww = sliding_window_weights('lin'+lineage_id, 500, 20, subsampling = subsample_numbers[lineage_id])\n",
    "    #swct = sliding_window_celltypes('lin'+lineage_id, 200, 10)\n",
    "    \n",
    "    centrality_heatmap = plot_centrality_and_weights(centrality_dict_downsampled, lineage,\n",
    "                                                    corr_threshold = CORR_THRESHOLD,\n",
    "                                                    degree_centrality_threshold = CENTRALITY_THRESHOLD,\n",
    "                                                    lineage_name = 'lin' + lineage_id,\n",
    "                                                    sww = sww)\n",
    "    centrality_dict_downsampled[lineage_id]['centrality_matrix'] = centrality_heatmap\n",
    "    print(len(centrality_heatmap))\n",
    "    # GET A NEW PATHWAY ACTIVITY THING FOR TEH DOWNSAMPLED STUFF HEHE\n",
    "    \n",
    "    lineage_pathway_activity_median = pd.read_csv(\"pseudotime_lineages/sliding_window/lin\" + lineage_id + '_v2_allcells/pathway_activity_subsampled_' + str(subsample_numbers[lineage_id]) +'_MEDIAN.csv',\n",
    "           index_col = 0)\n",
    "    lineage_pathway_activity_mean = pd.read_csv(\"pseudotime_lineages/sliding_window/lin\" + lineage_id + '_v2_allcells/pathway_activity_subsampled_' + str(subsample_numbers[lineage_id]) +'.csv',\n",
    "           index_col = 0)\n",
    "    \n",
    "    pathway_activity_windows_norm = lineage_pathway_activity_median.div(lineage_pathway_activity_median.max(axis=1), axis=0)\n",
    "    pathway_activity_windows_norm = pathway_activity_windows_norm.dropna(how='all')\n",
    "    \n",
    "    centrality_dict_downsampled[lineage_id]['activity_matrix_norm_median'] = pathway_activity_windows_norm\n",
    "    #centrality_dict_downsampled[lineage_id]['activity_matrix_median'] = lineage_pathway_activity\n",
    "    \n",
    "    pathway_activity_windows_norm = lineage_pathway_activity_mean.div(lineage_pathway_activity_mean.max(axis=1), axis=0)\n",
    "    pathway_activity_windows_norm = pathway_activity_windows_norm.dropna(how='all')\n",
    "        \n",
    "    centrality_dict_downsampled[lineage_id]['activity_matrix_norm_mean'] = pathway_activity_windows_norm\n",
    "    #centrality_dict_downsampled[lineage_id]['activity_matrix_mean'] = lineage_pathway_activity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
